# Feature Specification: [FEATURE NAME]

**Feature Branch**: `[###-feature-name]`
**Plugin**: [Ruinae / Iterum / KrateDSP / Shared] <!-- Which plugin or library this spec targets -->
**Created**: [DATE]
**Status**: Draft
**Input**: User description: "$ARGUMENTS"

## User Scenarios & Testing *(mandatory)*

<!--
  IMPORTANT: User stories should be PRIORITIZED as user journeys ordered by importance.
  Each user story/journey must be INDEPENDENTLY TESTABLE - meaning if you implement just ONE of them,
  you should still have a viable MVP (Minimum Viable Product) that delivers value.
  
  Assign priorities (P1, P2, P3, etc.) to each story, where P1 is the most critical.
  Think of each story as a standalone slice of functionality that can be:
  - Developed independently
  - Tested independently
  - Deployed independently
  - Demonstrated to users independently
-->

### User Story 1 - [Brief Title] (Priority: P1)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently - e.g., "Can be fully tested by [specific action] and delivers [specific value]"]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]
2. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

### User Story 2 - [Brief Title] (Priority: P2)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

### User Story 3 - [Brief Title] (Priority: P3)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

[Add more user stories as needed, each with an assigned priority]

### Edge Cases

<!--
  ACTION REQUIRED: The content in this section represents placeholders.
  Fill them out with the right edge cases.
-->

- What happens when [boundary condition]?
- How does system handle [error scenario]?

## Requirements *(mandatory)*

<!--
  ACTION REQUIRED: The content in this section represents placeholders.
  Fill them out with the right functional requirements.
-->

### Functional Requirements

- **FR-001**: System MUST [specific capability, e.g., "allow users to create accounts"]
- **FR-002**: System MUST [specific capability, e.g., "validate email addresses"]  
- **FR-003**: Users MUST be able to [key interaction, e.g., "reset their password"]
- **FR-004**: System MUST [data requirement, e.g., "persist user preferences"]
- **FR-005**: System MUST [behavior, e.g., "log all security events"]

*Example of marking unclear requirements:*

- **FR-006**: System MUST authenticate users via [NEEDS CLARIFICATION: auth method not specified - email/password, SSO, OAuth?]
- **FR-007**: System MUST retain user data for [NEEDS CLARIFICATION: retention period not specified]

### Key Entities *(include if feature involves data)*

- **[Entity 1]**: [What it represents, key attributes without implementation]
- **[Entity 2]**: [What it represents, relationships to other entities]

## Success Criteria *(mandatory)*

<!--
  ACTION REQUIRED: Define measurable success criteria.
  These must be technology-agnostic and measurable.
-->

### Measurable Outcomes

- **SC-001**: [Measurable metric, e.g., "Users can complete account creation in under 2 minutes"]
- **SC-002**: [Measurable metric, e.g., "System handles 1000 concurrent users without degradation"]
- **SC-003**: [User satisfaction metric, e.g., "90% of users successfully complete primary task on first attempt"]
- **SC-004**: [Business metric, e.g., "Reduce support tickets related to [X] by 50%"]

## Assumptions & Existing Components *(mandatory)*

<!--
  ACTION REQUIRED: Before proceeding to planning phase, identify existing codebase
  components that may be relevant to this feature. This prevents ODR violations
  and duplicate implementations per Constitution Principle XIV.
-->

### Assumptions

- [Assumption 1: e.g., "Users have accounts before using this feature"]
- [Assumption 2: e.g., "System has network connectivity"]

### Existing Codebase Components (Principle XIV)

*GATE: Must identify before `/speckit.plan` to prevent ODR violations.*

**Relevant existing components that may be reused or extended:**

| Component | Location | Relevance |
|-----------|----------|-----------|
| [e.g., existing Smoother class] | [e.g., dsp_utils.h] | [May conflict / Should reuse / Reference implementation] |

**Initial codebase search for key terms:**

```bash
# Run these searches to identify existing implementations
grep -r "[key term 1]" dsp/ plugins/
grep -r "[key term 2]" dsp/ plugins/
```

**Search Results Summary**: [Document what was found or "No existing implementations found"]

### Forward Reusability Consideration

*Note for planning phase: When this is a Layer 2+ feature, consider what new code might be reusable by sibling features at the same layer. The `/speckit.plan` phase will analyze this in detail, but early identification helps.*

**Sibling features at same layer** (if known):
- [e.g., Similar feature A that might share code]
- [e.g., Similar feature B that might share code]

**Potential shared components** (preliminary, refined in plan.md):
- [e.g., "MotorController might be useful for BBD mode too"]
- [e.g., "Composition pattern with FeedbackNetwork likely shared"]

## Implementation Verification *(mandatory at completion)*

<!--
  CRITICAL: This section MUST be completed when claiming spec completion.
  Constitution Principle XVI: Honest Completion requires explicit verification
  of ALL requirements before claiming "done".

  DO NOT fill this table from memory or assumptions. Each row requires you to
  re-read the actual implementation code and actual test output RIGHT NOW,
  then record what you found with specific file paths, line numbers, and
  measured values. Generic evidence like "implemented" or "test passes" is
  NOT acceptable ‚Äî it must be verifiable by a human reader.

  This section is EMPTY during specification phase and filled during
  implementation phase when /speckit.implement completes.
-->

### Compliance Status

*For EACH row below, you MUST perform these steps before writing the status:*
1. *Re-read the requirement from the spec*
2. *Open the implementation file and find the code that satisfies it ‚Äî record the file path and line number*
3. *Run or read the test that proves it ‚Äî record the test name and its actual output/result*
4. *For numeric thresholds (SC-xxx): record the actual measured value vs the spec target*
5. *Only then write the status and evidence*

*DO NOT mark ‚úÖ without having just verified the code and test output. DO NOT claim completion if ANY requirement is ‚ùå NOT MET without explicit user approval.*

| Requirement | Status | Evidence |
|-------------|--------|----------|
| FR-001 | [‚úÖ/‚ùå/‚ö†Ô∏è/üîÑ] | [`file.h:line` ‚Äî what it does; test `TestName` result: actual value] |
| FR-002 | [‚úÖ/‚ùå/‚ö†Ô∏è/üîÑ] | [`file.h:line` ‚Äî what it does; test `TestName` result: actual value] |
| SC-001 | [‚úÖ/‚ùå/‚ö†Ô∏è/üîÑ] | [Test `TestName`: measured X.XX vs target Y.YY] |
| SC-002 | [‚úÖ/‚ùå/‚ö†Ô∏è/üîÑ] | [Test `TestName`: measured X.XX vs target Y.YY] |

**Status Key:**
- ‚úÖ MET: Requirement verified against actual code and test output with specific evidence
- ‚ùå NOT MET: Requirement not satisfied (spec is NOT complete)
- ‚ö†Ô∏è PARTIAL: Partially met with documented gap and specific evidence of what IS met
- üîÑ DEFERRED: Explicitly moved to future work with user approval

### Completion Checklist

*All items must be checked before claiming completion:*

- [ ] Each FR-xxx row was verified by re-reading the actual implementation code (not from memory)
- [ ] Each SC-xxx row was verified by running tests or reading actual test output (not assumed)
- [ ] Evidence column contains specific file paths, line numbers, test names, and measured values
- [ ] No evidence column contains only generic claims like "implemented", "works", or "test passes"
- [ ] No test thresholds relaxed from spec requirements
- [ ] No placeholder values or TODO comments in new code
- [ ] No features quietly removed from scope
- [ ] User would NOT feel cheated by this completion claim

### Honest Assessment

**Overall Status**: [COMPLETE / NOT COMPLETE / PARTIAL]

**If NOT COMPLETE, document gaps:**
- [Gap 1: FR-xxx not met because...]
- [Gap 2: SC-xxx achieves X instead of Y because...]

**Recommendation**: [What needs to happen to achieve completion]
